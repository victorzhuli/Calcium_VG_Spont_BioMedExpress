{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to victor.li.zhu@rutgers.edu and will expire on October 06, 2017.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: /tmp/graphlab_server_1489596109.log\n"
     ]
    }
   ],
   "source": [
    "import graphlab as gl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import threading\n",
    "threading.activeCount()\n",
    "\n",
    "from ipyparallel import Client\n",
    "rc = Client()\n",
    "\n",
    "rc.ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Loading features: mouse05, window_length100 ...\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "Classifying --> classifier: KNN, feature_type001, k: 01 ...\n"
     ]
    }
   ],
   "source": [
    "# input parameters\n",
    "#=================================\n",
    "classifier_list = ['KNN']\n",
    "#=================================\n",
    "\n",
    "window_length_list  = [100, 150, 200, 250, 300]\n",
    "window_step = 50\n",
    "feature_type_list   = [1, 2, 3, 12, 13, 23, 123]  # 1: dg, 2: cc, 3: pl, 12: dg+cc, 13: dg+pl, 23: cc+pl, 123: dg+cc+pl\n",
    "k_list    = range(1,11)\n",
    "\n",
    "for mouse_id in range(5,6):\n",
    "\n",
    "    for win_ln_idx in range(len(window_length_list)): # NOTE HERE!!!!! REMOVE \"1\" AFTER THIS SESSION!!!!!!!!!!!!!\n",
    "        window_length = window_length_list[win_ln_idx]\n",
    "        \n",
    "        ##========================= load features\n",
    "        # print on screen: progress\n",
    "        current_progress = (\"\\n========================================\\nLoading features: mouse%02d, window_length%03d ...\\n\" \\\n",
    "                            % (mouse_id, window_length))\n",
    "        print current_progress\n",
    "\n",
    "        # load and organize feature matrix\n",
    "        comb = lz_load_feature_matrix(mouse_id, window_length, window_step)       \n",
    "        \n",
    "        # normalization\n",
    "        comb_normalization, norm = normalize_features_mice(comb)\n",
    "        \n",
    "        for classifier_idx in range(len(classifier_list)):\n",
    "            classifier = classifier_list[classifier_idx]\n",
    "\n",
    "            # initiate\n",
    "            AC = [[0] * len(k_list) for _ in range(len(feature_type_list))] \n",
    "            SE = [[0] * len(k_list) for _ in range(len(feature_type_list))] \n",
    "            SP = [[0] * len(k_list) for _ in range(len(feature_type_list))] \n",
    "            \n",
    "            for feature_type_idx in range(len(feature_type_list)):\n",
    "                feature_type = feature_type_list[feature_type_idx]             \n",
    "\n",
    "                ##========================= classification\n",
    "                for k_idx in range(len(k_list)):\n",
    "                    k = k_list[k_idx]                    \n",
    "\n",
    "                    # print on screen: progress\n",
    "                    current_progress = (\"\\n----------------------------------------\\nClassifying --> classifier: %s, feature_type%03d, k: %02d ...\" \\\n",
    "                                        % (classifier, feature_type, k))\n",
    "                    print current_progress\n",
    "\n",
    "                    if classifier == 'LR':\n",
    "                        AC[feature_type_idx][l2_penalty_idx],SE[feature_type_idx][l2_penalty_idx],SP[feature_type_idx][l2_penalty_idx] = \\\n",
    "                            lz_logistic_AC_SE_SP(comb, feature_type, l2_penalty, l1_penalty)\n",
    "                    if classifier == 'SVM':\n",
    "                        AC[feature_type_idx][l2_penalty_idx],SE[feature_type_idx][l2_penalty_idx],SP[feature_type_idx][l2_penalty_idx] = \\\n",
    "                            lz_svm_AC_SE_SP(comb, feature_type, l2_penalty)\n",
    "                    if classifier == 'RF':\n",
    "                        AC[feature_type_idx][subSample_idx],SE[feature_type_idx][subSample_idx],SP[feature_type_idx][subSample_idx] = \\\n",
    "                            lz_RF_AC_SE_SP(comb, feature_type, row_subSamp, col_subSamp)\n",
    "                    if classifier == 'KNN':\n",
    "                        AC[feature_type_idx][k_idx],SE[feature_type_idx][k_idx],SP[feature_type_idx][k_idx] = \\\n",
    "                            lz_KNN_AC_SE_SP(comb, feature_type, k)\n",
    "\n",
    "            ##====================== save results\n",
    "            AC_saveName = (\"%s_mouse%02d_window_length%03d_AC.csv\" \\\n",
    "                           % (classifier,mouse_id,window_length))\n",
    "            SE_saveName = (\"%s_mouse%02d_window_length%03d_SE.csv\" \\\n",
    "                           % (classifier,mouse_id,window_length))\n",
    "            SP_saveName = (\"%s_mouse%02d_window_length%03d_SP.csv\" \\\n",
    "                           % (classifier,mouse_id,window_length))\n",
    "            df_AC = pd.DataFrame(AC)\n",
    "            df_SE = pd.DataFrame(SE)        \n",
    "            df_SP = pd.DataFrame(SP)  \n",
    "            \n",
    "            os.chdir(\"/Users/lizhu/Dropbox/projects/calcium/classification_result\") \n",
    "            df_AC.to_csv(AC_saveName, index = False, header = False)\n",
    "            df_SE.to_csv(SE_saveName, index = False, header = False)\n",
    "            df_SP.to_csv(SP_saveName, index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load and organize feature matrix\n",
    "def lz_load_feature_matrix(mouse_id, window_length, window_step):\n",
    "    fileName = \"format4ML_GC6f_emx_0\" + str(mouse_id) + \"_windowLen\" + str(window_length) + \"_winStep_0\" + str(window_step) + \"_v2_threshold_10.csv\"\n",
    "    loadPath = \"/Users/lizhu/Dropbox/projects/calcium/format4ML/\" + fileName\n",
    "    comb = gl.SFrame.read_csv(loadPath, delimiter=',',header=False,verbose = False)\n",
    "    colName_dg = 'degree'\n",
    "    colName_dg = gl.SArray([colName_dg + repr(i+1) for i in range(30)])\n",
    "    colName_cc = 'clusteringCoef'\n",
    "    colName_cc = gl.SArray([colName_cc + repr(i+1) for i in range(30)])\n",
    "    colName_pl = 'pathlength'\n",
    "    colName_pl = gl.SArray([colName_pl + repr(i+1) for i in range(30)])\n",
    "    colName = colName_dg.append(colName_cc.append(colName_pl.append(gl.SArray(['Whisker']))))\n",
    "    colName = (list(colName))\n",
    "    dictionary = dict(zip(comb.column_names(), colName))\n",
    "    comb = comb.rename(dictionary)\n",
    "    comb = gl.toolkits.cross_validation.shuffle(comb, random_seed=1)\n",
    "    # comb['Whisker'].show(view = 'Categorical')\n",
    "    \n",
    "    return comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize the features\n",
    "# feature_matrix is a SFrame, first 90 columns are features, last column is labels.\n",
    "def normalize_features_mice(comb):\n",
    "    \n",
    "    colName = comb.column_names()\n",
    "    \n",
    "    # extract feature columns\n",
    "    comb_np = comb.to_numpy()\n",
    "    comb_feature = comb_np[:,0:90]\n",
    "    \n",
    "    # compute normed features and norms \n",
    "    norms = np.linalg.norm(comb_feature, axis=0) # gives [norm(X[:,0]), norm(X[:,1]), norm(X[:,2])]\n",
    "    comb_feature_normed = comb_feature / norms\n",
    "    \n",
    "    # combine normalized feature vectors and the labels\n",
    "    comb_normed = np.concatenate((comb_feature_normed,comb_np[:,90:94]),axis = 1)\n",
    "        \n",
    "    # convert numpy array to SFrame\n",
    "    comb_normed = gl.SFrame(comb_normed)\n",
    "    comb_normed = comb_normed.unpack('X1', '')\n",
    "\n",
    "    dictionary_norm = dict(zip(comb_normed.column_names(), colName))\n",
    "    comb_normed = comb_normed.rename(dictionary_norm) # this is the normed version\n",
    "\n",
    "    return comb_normed, norms # note: norms is array, comb_normed is SFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train and cross-validation\n",
    "def lz_KNN_AC_SE_SP(data, feature_type, k):\n",
    "    \n",
    "    # clearify features\n",
    "    feature_dg = comb.column_names()[0:30] # feature 1~30: degree, 31~60: clustering coefficient, 60~90: pathlength\n",
    "    feature_cc = comb.column_names()[30:60]\n",
    "    feature_pl = comb.column_names()[60:90]\n",
    "    \n",
    "    if feature_type == 1:   feature = feature_dg\n",
    "    if feature_type == 2:   feature = feature_cc\n",
    "    if feature_type == 3:   feature = feature_pl\n",
    "    if feature_type == 12:  feature = feature_dg + feature_cc\n",
    "    if feature_type == 13:  feature = feature_dg + feature_pl\n",
    "    if feature_type == 23:  feature = feature_cc + feature_pl\n",
    "    if feature_type == 123: feature = feature_dg + feature_cc + feature_pl\n",
    "     \n",
    "    # Kfold\n",
    "    num_fold = 10\n",
    "    folds = gl.cross_validation.KFold(comb, num_fold)\n",
    "    SE = [None] * num_fold\n",
    "    SP = [None] * num_fold\n",
    "    AC = [None] * num_fold\n",
    "    # print specificity\n",
    "    idx = 0\n",
    "    for train, valid in folds:\n",
    "        m = gl.nearest_neighbor_classifier.create(train,\n",
    "                                          target = 'Whisker',\n",
    "                                          features = feature,\n",
    "                                          distance = 'euclidean',\n",
    "                                          verbose = False)\n",
    "        confusion_matrix = m.evaluate(valid, 'confusion_matrix', max_neighbors = k)\n",
    "        confusion_matrix = confusion_matrix.values()[0]\n",
    "        \n",
    "        TP, TN, FP, FN = lz_extract_ACC_SE_SP(confusion_matrix)\n",
    "        \n",
    "        SP[idx] = float(TN) / (TN + FP)\n",
    "        SE[idx] = float(TP) / (TP + FN)\n",
    "        AC[idx] = float(TP+TN) / (TP+TN+FP+FN)\n",
    "        idx = idx + 1\n",
    "        \n",
    "    AC_mn = np.mean(AC)\n",
    "    AC_sd = np.std(AC)\n",
    "    SE_mn = np.mean(SE)\n",
    "    SE_sd = np.std(SE)\n",
    "    SP_mn = np.mean(SP)\n",
    "    SP_sd = np.std(SP)\n",
    "    \n",
    "    print 'AC_mean = ', AC_mn\n",
    "    print 'AC_std = ',  AC_sd\n",
    "    print 'SE_mean = ', SE_mn\n",
    "    print 'SE_std = ',  SE_sd\n",
    "    print 'SP_mean = ', SP_mn\n",
    "    print 'SP_std = ',  SP_sd\n",
    "    \n",
    "    return AC_mn, SE_mn, SP_mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lz_extract_ACC_SE_SP(confusion_matrix):\n",
    "    TP = confusion_matrix[(confusion_matrix['target_label']==1) & (confusion_matrix['predicted_label']==1)]\n",
    "    if np.size(TP) == 0:\n",
    "        TP = 0\n",
    "    else:\n",
    "        TP = TP['count'][0]\n",
    "    TN = confusion_matrix[(confusion_matrix['target_label']==0) & (confusion_matrix['predicted_label']==0)]\n",
    "    if np.size(TN) == 0:\n",
    "        TN = 0\n",
    "    else:\n",
    "        TN = TN['count'][0]\n",
    "    FP = confusion_matrix[(confusion_matrix['target_label']==0) & (confusion_matrix['predicted_label']==1)]\n",
    "    if np.size(FP) == 0:\n",
    "        FP = 0\n",
    "    else:\n",
    "        FP = FP['count'][0]\n",
    "    FN = confusion_matrix[(confusion_matrix['target_label']==1) & (confusion_matrix['predicted_label']==0)]\n",
    "    if np.size(FN) == 0:\n",
    "        FN = 0\n",
    "    else:\n",
    "        FN = FN['count'][0]\n",
    "    \n",
    "    return TP, TN, FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############### this is the normalized version of the above code\n",
    "\n",
    "# normalize the features\n",
    "# feature_matrix is a SFrame, first 90 columns are features, last column is labels.\n",
    "def normalize_features_mice(comb):\n",
    "    # extract feature columns\n",
    "    comb_np = comb.to_numpy()\n",
    "    comb_feature = comb_np[:,0:90]\n",
    "    \n",
    "    # compute normed features and norms \n",
    "    norms = np.linalg.norm(comb_feature, axis=0) # gives [norm(X[:,0]), norm(X[:,1]), norm(X[:,2])]\n",
    "    comb_feature_normed = comb_feature / norms\n",
    "    \n",
    "    # combine normalized feature vectors and the labels\n",
    "    comb_normed = np.concatenate((comb_feature_normed,comb_np[:,90:91]),axis = 1)\n",
    "    # convert numpy array to SFrame\n",
    "    comb_normed = gl.SFrame(comb_normed)\n",
    "    comb_normed = comb_normed.unpack('X1', '')\n",
    "    dictionary_norm = dict(zip(comb_normed.column_names(), colName))\n",
    "    comb_normed = comb_normed.rename(dictionary_norm) # this is the normed version\n",
    "\n",
    "    return comb_normed, norms # note: norms is array, comb_normed is SFrame\n",
    "\n",
    "# comb = gl.toolkits.cross_validation.shuffle(comb, random_seed=2)\n",
    "def lz_logistic(data, feature):\n",
    "    \n",
    "    # normalize \n",
    "    comb, norms = normalize_features_mice(data)\n",
    "    comb['Whisker'] = comb['Whisker'].astype(int)\n",
    "\n",
    "    folds = gl.cross_validation.KFold(comb, 5)\n",
    "    SE=[None]*5\n",
    "    SP=[None]*5\n",
    "    acc=[None]*5\n",
    "    # print specificity\n",
    "    idx = 0\n",
    "    for train, valid in folds:\n",
    "        m = gl.logistic_classifier.create(train,\n",
    "                                          target='Whisker',\n",
    "                                          features=feature,\n",
    "                                          l2_penalty = 0.05,\n",
    "                                          validation_set=None,\n",
    "                                          verbose = False)\n",
    "        \n",
    "        # normalize the validation dataset\n",
    "        valid_np = valid.to_numpy()\n",
    "        valid_feature = valid_np[:,0:90]\n",
    "\n",
    "        # use normed from features\n",
    "        valid_feature_normed = valid_feature / norms\n",
    "\n",
    "        # combine normalized feature vectors and the labels\n",
    "        valid_normed = np.concatenate((valid_feature_normed,valid_np[:,90:91]),axis = 1)\n",
    "\n",
    "        # convert numpy array to SFrame\n",
    "        valid_normed = gl.SFrame(valid_normed)\n",
    "        valid_normed = valid_normed.unpack('X1', '')\n",
    "        dictionary_norm = dict(zip(valid_normed.column_names(), colName))\n",
    "        valid_normed = valid_normed.rename(dictionary_norm) # this is the normed version\n",
    "        valid_normed['Whisker'] = valid_normed['Whisker'].astype(int)\n",
    "\n",
    "        confusion_matrix = m.evaluate(valid, 'confusion_matrix')\n",
    "        confusion_matrix = confusion_matrix.values()[0]\n",
    "        TP = confusion_matrix[(confusion_matrix['target_label']==1) & (confusion_matrix['predicted_label']==1)]\n",
    "        if np.size(TP) == 0:\n",
    "            TP = 0\n",
    "        else:\n",
    "            TP = TP['count'][0]\n",
    "        TN = confusion_matrix[(confusion_matrix['target_label']==0) & (confusion_matrix['predicted_label']==0)]\n",
    "        if np.size(TN) == 0:\n",
    "            TN = 0\n",
    "        else:\n",
    "            TN = TN['count'][0]\n",
    "        FP = confusion_matrix[(confusion_matrix['target_label']==0) & (confusion_matrix['predicted_label']==1)]\n",
    "        if np.size(FP) == 0:\n",
    "            FP = 0\n",
    "        else:\n",
    "            FP = FP['count'][0]\n",
    "        FN = confusion_matrix[(confusion_matrix['target_label']==1) & (confusion_matrix['predicted_label']==0)]\n",
    "        if np.size(FN) == 0:\n",
    "            FN = 0\n",
    "        else:\n",
    "            FN = FN['count'][0]\n",
    "        SE[idx] = float(TN) / (TN + FP)\n",
    "        SP[idx] = float(TP) / (TP + FN)\n",
    "        acc[idx] = float(TP+TN) / (TP+TN+FP+FN)\n",
    "        idx = idx + 1\n",
    "    \n",
    "    print 'acc_mean = ', np.mean(acc)\n",
    "    print 'acc_std = ', np.std(acc)\n",
    "    print 'SE_mean = ', np.mean(SE)\n",
    "    print 'SE_std = ', np.std(SE)\n",
    "    print 'SP_mean = ', np.mean(SP)\n",
    "    print 'SP_std = ', np.std(SP)\n",
    "    \n",
    "    return acc, SE, SP\n",
    "\n",
    "# normalize\n",
    "\n",
    "feature = comb.column_names()[60:90] \n",
    "\n",
    "acc,SE,SP = lz_logistic(comb, feature)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# exploring the data\n",
    "num_wm = len(comb[comb['Whisker']==1])\n",
    "num_wn = len(comb[comb['Whisker']==0])\n",
    "# major class classifier(naive classifier)\n",
    "major_classifier = float(num_wn)/(num_wm + num_wn)\n",
    "major_classifier\n",
    "num_wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data,test_data = comb.random_split(.8, seed=0)\n",
    "# train_valid_shuffled = graphlab.toolkits.cross_validation.shuffle(train_valid, random_seed=1)\n",
    "train_data = gl.toolkits.cross_validation.shuffle(train_data, random_seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "folds = gl.cross_validation.KFold(comb, 5)\n",
    "folds\n",
    "params = dict([('target', 'Whisker'), ('features', comb.column_names()[0:90]), ('validation_set', None)])\n",
    "\n",
    "all_feature_model = gl.cross_validation.cross_val_score(folds,\n",
    "                                              gl.logistic_classifier.create,\n",
    "                                              params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print all_feature_model.get_results()\n",
    "all_feature_model.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mylist = sum([range(0,30)] + [range(60,90)],[])\n",
    "mylist[0:3,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(colName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "whisker_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Weight \n",
    "weight = whisker_model['coefficients'].sort('value', ascending=False)\n",
    "weight['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
