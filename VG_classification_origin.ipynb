{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to victor.li.zhu@rutgers.edu and will expire on October 06, 2017.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: /tmp/graphlab_server_1489253713.log\n"
     ]
    }
   ],
   "source": [
    "import graphlab as gl\n",
    "import numpy as np\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sig(t):\n",
    "    sig = 1. / (1+np.exp(-t))\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_numpy_data(data_sframe, features, label):\n",
    "    data_sframe['intercept'] = 1\n",
    "    features = ['intercept'] + features\n",
    "    features_sframe = data_sframe[features]\n",
    "    feature_matrix = features_sframe.to_numpy()\n",
    "    label_sarray = data_sframe[label]\n",
    "    label_array = label_sarray.to_numpy()\n",
    "    return(feature_matrix, label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comb = gl.SFrame.read_csv('/Users/lizhu/Dropbox/projects/calcium/format4ML/format4ML_GC6f_emx_01_windowLen200_winStep_050_v2_threshold_10.csv', delimiter=',',header=False,verbose = False)\n",
    "colName_dg = 'degree'\n",
    "colName_dg = gl.SArray([colName_dg + repr(i+1) for i in range(30)])\n",
    "\n",
    "colName_cc = 'clusteringCoef'\n",
    "colName_cc = gl.SArray([colName_cc + repr(i+1) for i in range(30)])\n",
    "\n",
    "colName_pl = 'pathlength'\n",
    "colName_pl = gl.SArray([colName_pl + repr(i+1) for i in range(30)])\n",
    "\n",
    "colName = colName_dg.append(colName_cc.append(colName_pl.append(gl.SArray(['Whisker']))))\n",
    "colName = (list(colName))\n",
    "# type(colName)\n",
    "\n",
    "dictionary = dict(zip(comb.column_names(), colName))\n",
    "comb = comb.rename(dictionary)\n",
    "comb = gl.toolkits.cross_validation.shuffle(comb, random_seed=1)\n",
    "\n",
    "# comb['Whisker'].show(view = 'Categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_mean =  0.888498789346\n",
      "acc_std =  0.0363610479115\n",
      "SE_mean =  0.671912353205\n",
      "SE_std =  0.103620159816\n",
      "SP_mean =  0.946521169739\n",
      "SP_std =  0.0245938233062\n"
     ]
    }
   ],
   "source": [
    "# NOTEï¼šrun the functions below before running this section!!!\n",
    "\n",
    "feature_dg = comb.column_names()[0:30] # feature 1~30: degree, 31~60: clustering coefficient, 60~90: pathlength\n",
    "feature_cc = comb.column_names()[30:60]\n",
    "feature_pl = comb.column_names()[60:90]\n",
    "\n",
    "l2_penalty = 0.01\n",
    "l1_penalty = 0\n",
    "\n",
    "acc,SE,SP = lz_logistic_acc_SE_SP(comb, feature_dg, l2_penalty, l1_penalty)\n",
    "# acc,SE,SP = lz_logistic_acc_SE_SP(comb, feature_dg+feature_pl+feature_cc, l2_penalty, l1_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############### this results have been used for CLEO paper\n",
    "\n",
    "# comb = gl.toolkits.cross_validation.shuffle(comb, random_seed=2)\n",
    "\n",
    "def lz_logistic_acc_SE_SP(data, feature, l2_penalty, l1_penalty):\n",
    "    \n",
    "    folds = gl.cross_validation.KFold(comb, 10)\n",
    "    SE=[None]*10\n",
    "    SP=[None]*10\n",
    "    acc=[None]*10\n",
    "    # print specificity\n",
    "    idx = 0\n",
    "    for train, valid in folds:\n",
    "        m = gl.logistic_classifier.create(train,\n",
    "                                          target='Whisker',\n",
    "                                          features=feature,\n",
    "                                          l2_penalty = l2_penalty, \n",
    "                                          l1_penalty = l1_penalty,\n",
    "                                          validation_set=None,\n",
    "                                          verbose = False)\n",
    "        confusion_matrix = m.evaluate(valid, 'confusion_matrix')\n",
    "        confusion_matrix = confusion_matrix.values()[0]\n",
    "        \n",
    "        TP, TN, FP, FN = lz_extract_ACC_SE_SP(confusion_matrix)\n",
    "        \n",
    "        SP[idx] = float(TN) / (TN + FP)\n",
    "        SE[idx] = float(TP) / (TP + FN)\n",
    "        acc[idx] = float(TP+TN) / (TP+TN+FP+FN)\n",
    "        idx = idx + 1\n",
    "    \n",
    "    print 'acc_mean = ', np.mean(acc)\n",
    "    print 'acc_std = ', np.std(acc)\n",
    "    print 'SE_mean = ', np.mean(SE)\n",
    "    print 'SE_std = ', np.std(SE)\n",
    "    print 'SP_mean = ', np.mean(SP)\n",
    "    print 'SP_std = ', np.std(SP)\n",
    "    \n",
    "    return acc, SE, SP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lz_extract_ACC_SE_SP(confusion_matrix):\n",
    "    TP = confusion_matrix[(confusion_matrix['target_label']==1) & (confusion_matrix['predicted_label']==1)]\n",
    "    if np.size(TP) == 0:\n",
    "        TP = 0\n",
    "    else:\n",
    "        TP = TP['count'][0]\n",
    "    TN = confusion_matrix[(confusion_matrix['target_label']==0) & (confusion_matrix['predicted_label']==0)]\n",
    "    if np.size(TN) == 0:\n",
    "        TN = 0\n",
    "    else:\n",
    "        TN = TN['count'][0]\n",
    "    FP = confusion_matrix[(confusion_matrix['target_label']==0) & (confusion_matrix['predicted_label']==1)]\n",
    "    if np.size(FP) == 0:\n",
    "        FP = 0\n",
    "    else:\n",
    "        FP = FP['count'][0]\n",
    "    FN = confusion_matrix[(confusion_matrix['target_label']==1) & (confusion_matrix['predicted_label']==0)]\n",
    "    if np.size(FN) == 0:\n",
    "        FN = 0\n",
    "    else:\n",
    "        FN = FN['count'][0]\n",
    "    \n",
    "    return TP, TN, FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############### this is the normalized version of the above code\n",
    "\n",
    "# normalize the features\n",
    "# feature_matrix is a SFrame, first 90 columns are features, last column is labels.\n",
    "def normalize_features_mice(comb):\n",
    "    # extract feature columns\n",
    "    comb_np = comb.to_numpy()\n",
    "    comb_feature = comb_np[:,0:90]\n",
    "    \n",
    "    # compute normed features and norms \n",
    "    norms = np.linalg.norm(comb_feature, axis=0) # gives [norm(X[:,0]), norm(X[:,1]), norm(X[:,2])]\n",
    "    comb_feature_normed = comb_feature / norms\n",
    "    \n",
    "    # combine normalized feature vectors and the labels\n",
    "    comb_normed = np.concatenate((comb_feature_normed,comb_np[:,90:91]),axis = 1)\n",
    "    # convert numpy array to SFrame\n",
    "    comb_normed = gl.SFrame(comb_normed)\n",
    "    comb_normed = comb_normed.unpack('X1', '')\n",
    "    dictionary_norm = dict(zip(comb_normed.column_names(), colName))\n",
    "    comb_normed = comb_normed.rename(dictionary_norm) # this is the normed version\n",
    "\n",
    "    return comb_normed, norms # note: norms is array, comb_normed is SFrame\n",
    "\n",
    "# comb = gl.toolkits.cross_validation.shuffle(comb, random_seed=2)\n",
    "def lz_logistic(data, feature):\n",
    "    \n",
    "    # normalize \n",
    "    comb, norms = normalize_features_mice(data)\n",
    "    comb['Whisker'] = comb['Whisker'].astype(int)\n",
    "\n",
    "    folds = gl.cross_validation.KFold(comb, 5)\n",
    "    SE  = [None]*5\n",
    "    SP  = [None]*5\n",
    "    acc = [None]*5\n",
    "    # print specificity\n",
    "    idx = 0\n",
    "    for train, valid in folds:\n",
    "        m = gl.logistic_classifier.create(train,\n",
    "                                          target='Whisker',\n",
    "                                          features=feature,\n",
    "                                          l2_penalty = 0.05,\n",
    "                                          validation_set=None,\n",
    "                                          verbose = False)\n",
    "        \n",
    "        # normalize the validation dataset\n",
    "        valid_np = valid.to_numpy()\n",
    "        valid_feature = valid_np[:,0:90]\n",
    "\n",
    "        # use normed from features\n",
    "        valid_feature_normed = valid_feature / norms\n",
    "\n",
    "        # combine normalized feature vectors and the labels\n",
    "        valid_normed = np.concatenate((valid_feature_normed,valid_np[:,90:91]),axis = 1)\n",
    "\n",
    "        # convert numpy array to SFrame\n",
    "        valid_normed = gl.SFrame(valid_normed)\n",
    "        valid_normed = valid_normed.unpack('X1', '')\n",
    "        dictionary_norm = dict(zip(valid_normed.column_names(), colName))\n",
    "        valid_normed = valid_normed.rename(dictionary_norm) # this is the normed version\n",
    "        valid_normed['Whisker'] = valid_normed['Whisker'].astype(int)\n",
    "\n",
    "        confusion_matrix = m.evaluate(valid, 'confusion_matrix')\n",
    "        confusion_matrix = confusion_matrix.values()[0]\n",
    "        TP = confusion_matrix[(confusion_matrix['target_label']==1) & (confusion_matrix['predicted_label']==1)]\n",
    "        if np.size(TP) == 0:\n",
    "            TP = 0\n",
    "        else:\n",
    "            TP = TP['count'][0]\n",
    "        TN = confusion_matrix[(confusion_matrix['target_label']==0) & (confusion_matrix['predicted_label']==0)]\n",
    "        if np.size(TN) == 0:\n",
    "            TN = 0\n",
    "        else:\n",
    "            TN = TN['count'][0]\n",
    "        FP = confusion_matrix[(confusion_matrix['target_label']==0) & (confusion_matrix['predicted_label']==1)]\n",
    "        if np.size(FP) == 0:\n",
    "            FP = 0\n",
    "        else:\n",
    "            FP = FP['count'][0]\n",
    "        FN = confusion_matrix[(confusion_matrix['target_label']==1) & (confusion_matrix['predicted_label']==0)]\n",
    "        if np.size(FN) == 0:\n",
    "            FN = 0\n",
    "        else:\n",
    "            FN = FN['count'][0]\n",
    "        SE[idx] = float(TN) / (TN + FP)\n",
    "        SP[idx] = float(TP) / (TP + FN)\n",
    "        acc[idx] = float(TP+TN) / (TP+TN+FP+FN)\n",
    "        idx = idx + 1\n",
    "    \n",
    "    print 'acc_mean = ', np.mean(acc)\n",
    "    print 'acc_std = ', np.std(acc)\n",
    "    print 'SE_mean = ', np.mean(SE)\n",
    "    print 'SE_std = ', np.std(SE)\n",
    "    print 'SP_mean = ', np.mean(SP)\n",
    "    print 'SP_std = ', np.std(SP)\n",
    "    \n",
    "    return acc, SE, SP\n",
    "\n",
    "# normalize\n",
    "\n",
    "feature = comb.column_names()[60:90] \n",
    "\n",
    "acc,SE,SP = lz_logistic(comb, feature)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# exploring the data\n",
    "num_wm = len(comb[comb['Whisker']==1])\n",
    "num_wn = len(comb[comb['Whisker']==0])\n",
    "# major class classifier(naive classifier)\n",
    "major_classifier = float(num_wn)/(num_wm + num_wn)\n",
    "major_classifier\n",
    "num_wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data,test_data = comb.random_split(.8, seed=0)\n",
    "# train_valid_shuffled = graphlab.toolkits.cross_validation.shuffle(train_valid, random_seed=1)\n",
    "train_data = gl.toolkits.cross_validation.shuffle(train_data, random_seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "folds = gl.cross_validation.KFold(comb, 5)\n",
    "folds\n",
    "params = dict([('target', 'Whisker'), ('features', comb.column_names()[0:90]), ('validation_set', None)])\n",
    "\n",
    "all_feature_model = gl.cross_validation.cross_val_score(folds,\n",
    "                                              gl.logistic_classifier.create,\n",
    "                                              params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print all_feature_model.get_results()\n",
    "all_feature_model.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mylist = sum([range(0,30)] + [range(60,90)],[])\n",
    "mylist[0:3,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(colName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "whisker_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Weight \n",
    "weight = whisker_model['coefficients'].sort('value', ascending=False)\n",
    "weight['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
